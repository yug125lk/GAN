{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#globbing utility. upsamle discriminator\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load, split and scale the maps dataset ready for training\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from numpy import savez_compressed\n",
    "import random\n",
    "\n",
    "# example of pix2pix gan for satellite to map image-to-image translation\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/a']"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = './dataset/'\n",
    "lst = [os.path.join(d, o) for o in os.listdir(d) \n",
    "                    if os.path.isdir(os.path.join(d,o))]\n",
    "\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "./dataset/a/a_01.png\n",
      "./dataset/a/side/*.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "src = []\n",
    "tgt = []\n",
    "P = []\n",
    "for path in tqdm(lst):\n",
    "    path1 = path +\"/side/*.png\"\n",
    "    folder = path.split('/')[-1]\n",
    "    tgtpath = path+\"/\"+folder+\"_01.png\"\n",
    "    #print(tgtpath)\n",
    "    for file in glob.glob(path1):\n",
    "        #print (file)\n",
    "        a= cv2.imread(file)\n",
    "        a= cv2.resize(a, (256,256), interpolation = cv2.INTER_AREA)\n",
    "        src.append(a)\n",
    "        b= cv2.imread(tgtpath)\n",
    "        b= cv2.resize(b, (256,256), interpolation = cv2.INTER_AREA)\n",
    "        tgt.append(b)\n",
    "        \n",
    "\n",
    "print(folder)\n",
    "print(tgtpath)\n",
    "print(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256, 256, 3)\n",
      "(4, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "src = np.asarray(src)\n",
    "tgt = np.asarray(tgt)\n",
    "\n",
    "print(src.shape)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset:  multi.npz\n"
     ]
    }
   ],
   "source": [
    "filename = 'multi.npz'\n",
    "savez_compressed(filename, src, tgt)\n",
    "print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load('multi.npz')\n",
    "src_images, tar_images = data['arr_0'], data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (4, 256, 256, 3) (4, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the prepared dataset\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "data = load('multi.npz')\n",
    "src_images, tar_images = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# # plot source images\n",
    "# n_samples = 3\n",
    "# r = random.randint(0,len(src_images))\n",
    "# for i in range(n_samples):\n",
    "# \tpyplot.subplot(2, n_samples, 1 + i)\n",
    "# \tpyplot.axis('off')\n",
    "# \tpyplot.imshow(src_images[r+i].astype('uint8'))\n",
    "# # plot target image\n",
    "# for i in range(n_samples):\n",
    "# \tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "# \tpyplot.axis('off')\n",
    "# \tpyplot.imshow(tar_images[r+i].astype('uint8'))\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_122\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_181 (InputLayer)          [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_182 (InputLayer)          [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_295 (Concatenate)   (None, 256, 256, 6)  0           input_181[0][0]                  \n",
      "                                                                 input_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 128, 128, 64) 6208        concatenate_295[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_541 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 64, 64, 128)  131200      leaky_re_lu_541[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 64, 64, 128)  512         conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_542 (LeakyReLU)     (None, 64, 64, 128)  0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 32, 32, 256)  524544      leaky_re_lu_542[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 32, 32, 256)  1024        conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_543 (LeakyReLU)     (None, 32, 32, 256)  0           batch_normalization_692[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 16, 16, 512)  2097664     leaky_re_lu_543[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 16, 16, 512)  2048        conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_544 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 16, 16, 512)  4194816     leaky_re_lu_544[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 16, 16, 512)  2048        conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_545 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 16, 16, 1)    8193        leaky_re_lu_545[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 16, 16, 1)    0           conv2d_639[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,968,257\n",
      "Trainable params: 6,965,441\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_src_image = Input(shape=image_shape)\n",
    "\t# target image input\n",
    "\tin_target_image = Input(shape=image_shape)\n",
    "\t# concatenate images channel-wise\n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "\t# C64\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\treturn model\n",
    "model = define_discriminator((256, 256, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose\n",
    "\n",
    "# def define_discriminator(image_shape):\n",
    "#     # weight initialization\n",
    "#     init = RandomNormal(stddev=0.02)\n",
    "#     # source image input\n",
    "#     in_src_image = Input(shape=image_shape)\n",
    "#     # target image input\n",
    "#     in_target_image = Input(shape=image_shape)\n",
    "#     # concatenate images channel-wise\n",
    "#     merged = Concatenate()([in_src_image, in_target_image])\n",
    "#     # C64\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "#     d = LeakyReLU(alpha=0.2)(d)\n",
    "#     # C128\n",
    "#     d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     d = LeakyReLU(alpha=0.2)(d)\n",
    "#     # C256\n",
    "#     d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     d = LeakyReLU(alpha=0.2)(d)\n",
    "#     # C512\n",
    "#     d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     d = LeakyReLU(alpha=0.2)(d)\n",
    "#     # second last output layer\n",
    "#     d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     d = LeakyReLU(alpha=0.2)(d)\n",
    "#     # reshape output\n",
    "#     d = Conv2DTranspose(256, (4,4), strides=(4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     d = Conv2DTranspose(128, (4,4), strides=(4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     # patch output\n",
    "#     d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     patch_out = Activation('sigmoid')(d)\n",
    "#     # define model\n",
    "#     model = Model([in_src_image, in_target_image], patch_out)\n",
    "#     # compile model\n",
    "#     opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "#     return model\n",
    "# model = define_discriminator((256, 256, 3))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Add , UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "# def build_discriminator(input_shape=(256, 256, 3)):\n",
    "#     # Input layer\n",
    "#     input_img = Input(shape=input_shape)\n",
    "\n",
    "#     # Downsampling\n",
    "#     d1 = Conv2D(64, kernel_size=4, strides=2, padding='same')(input_img)\n",
    "#     d1 = LeakyReLU(alpha=0.2)(d1)\n",
    "\n",
    "#     d2 = Conv2D(128, kernel_size=4, strides=2, padding='same')(d1)\n",
    "#     d2 = BatchNormalization()(d2)\n",
    "#     d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "\n",
    "#     d3 = Conv2D(256, kernel_size=4, strides=2, padding='same')(d2)\n",
    "#     d3 = BatchNormalization()(d3)\n",
    "#     d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "\n",
    "#     d4 = Conv2D(512, kernel_size=4, strides=2, padding='same')(d3)\n",
    "#     d4 = BatchNormalization()(d4)\n",
    "#     d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "\n",
    "#     # Upsampling\n",
    "#     u1 = UpSampling2D(size=(2, 2))(d4)\n",
    "#     u1 = Conv2D(256, kernel_size=4, strides=1, padding='same')(u1)\n",
    "#     u1 = BatchNormalization()(u1)\n",
    "#     u1 = LeakyReLU(alpha=0.2)(u1)\n",
    "\n",
    "#     u2 = UpSampling2D(size=(2, 2))(u1)\n",
    "#     u2 = Conv2D(128, kernel_size=4, strides=1, padding='same')(u2)\n",
    "#     u2 = BatchNormalization()(u2)\n",
    "#     u2 = LeakyReLU(alpha=0.2)(u2)\n",
    "\n",
    "#     u3 = UpSampling2D(size=(2, 2))(u2)\n",
    "#     u3 = Conv2D(64, kernel_size=4, strides=1, padding='same')(u3)\n",
    "#     u3 = BatchNormalization()(u3)\n",
    "#     u3 = LeakyReLU(alpha=0.2)(u3)\n",
    "\n",
    "#     # Output layer\n",
    "#     output = Conv2D(1, kernel_size=4, strides=1, padding='same')(u3)\n",
    "\n",
    "#     return Model(input_img, output)\n",
    "\n",
    "\n",
    "# model2 = build_discriminator(input_shape=(256, 256, 3))\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_123\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_183 (InputLayer)          [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_184 (InputLayer)          [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_296 (Concatenate)   (None, 256, 256, 6)  0           input_183[0][0]                  \n",
      "                                                                 input_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_640 (Conv2D)             (None, 128, 128, 64) 6208        concatenate_296[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_546 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_641 (Conv2D)             (None, 64, 64, 128)  131200      leaky_re_lu_546[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_695 (BatchN (None, 64, 64, 128)  512         conv2d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_547 (LeakyReLU)     (None, 64, 64, 128)  0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_642 (Conv2D)             (None, 32, 32, 256)  524544      leaky_re_lu_547[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 32, 32, 256)  1024        conv2d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_548 (LeakyReLU)     (None, 32, 32, 256)  0           batch_normalization_696[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_643 (Conv2D)             (None, 16, 16, 512)  2097664     leaky_re_lu_548[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 16, 16, 512)  2048        conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_549 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_644 (Conv2D)             (None, 16, 16, 512)  4194816     leaky_re_lu_549[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 16, 16, 512)  2048        conv2d_644[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_550 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_645 (Conv2D)             (None, 16, 16, 1)    8193        leaky_re_lu_550[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 16, 16, 1)    0           conv2d_645[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,968,257\n",
      "Trainable params: 6,965,441\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model = define_discriminator((256, 256, 3))\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# encoder model\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "\te2 = define_encoder_block(e1, 128)\n",
    "\te3 = define_encoder_block(e2, 256)\n",
    "\te4 = define_encoder_block(e3, 512)\n",
    "\te5 = define_encoder_block(e4, 512)\n",
    "\te6 = define_encoder_block(e5, 512)\n",
    "\te7 = define_encoder_block(e6, 512)\n",
    "\t# bottleneck, no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "\tb = Activation('relu')(b)\n",
    "\t# decoder model\n",
    "\td1 = decoder_block(b, e7, 512)\n",
    "\td2 = decoder_block(d1, e6, 512)\n",
    "\td3 = decoder_block(d2, e5, 512)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\t# output\n",
    "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "g_model = define_generator((256, 256, 3))\n",
    "# plot_model(g_model, to_file='gen_model.png', show_shapes=True)\n",
    "# g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "\n",
    "\n",
    "def perceptual_loss(in_src, gen_out):\n",
    "\n",
    "    # Define a pre-trained VGG16 model without the last few layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Define a new model that outputs the feature maps of the VGG16 model\n",
    "    modelvgg = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_conv3').output)\n",
    "    \n",
    "    # Freeze the weights of the VGG16 model\n",
    "    for layer in modelvgg.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Define the perceptual loss function\n",
    "    def ploss(in_src, gen_out):\n",
    "        in_src = (in_src + 1) * 127.5\n",
    "        gen_out = (gen_out + 1) * 127.5\n",
    "        in_src = Lambda(lambda x: (x - 120.0) / 255.0)(in_src)  # normalize input image for VGG16\n",
    "        gen_out = Lambda(lambda x: (x - 120.0) / 255.0)(gen_out)\n",
    "        feature_true = modelvgg(in_src)\n",
    "        feature_pred = modelvgg(gen_out)\n",
    "        loss = K.mean(K.square(feature_true - feature_pred))\n",
    "        return loss\n",
    "    \n",
    "    return ploss(in_src, gen_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\td_model.trainable = False\n",
    "\t# define the source image\n",
    "\tin_src = Input(shape=image_shape)\n",
    "\t# connect the source image to the generator input\n",
    "\tgen_out = g_model(in_src)\n",
    "\t# connect the source input and generator output to the discriminator input\n",
    "\tdis_out = d_model([in_src, gen_out])\n",
    "\t# src image as input, generated image and classification output\n",
    "\tmodel = Model(in_src, [dis_out, gen_out])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=[perceptual_loss , 'binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "\treturn model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_125\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_186 (InputLayer)          [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_124 (Functional)          (None, 256, 256, 3)  54429315    input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_123 (Functional)          (None, 16, 16, 1)    6968257     input_186[0][0]                  \n",
      "                                                                 model_124[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 61,397,572\n",
      "Trainable params: 54,419,459\n",
      "Non-trainable params: 6,978,113\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_gan(g_model , d_model, (256, 256, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(filename):\n",
    "\t# load compressed arrays\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX1 = (X1 - 255.5) / 255.5\n",
    "\tX2 = (X2 - 255.5) / 255.5\n",
    "\treturn [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2 = trainA[ix], trainB[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(samples)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "\t# select a sample of input images\n",
    "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "\t# generate a batch of fake samples\n",
    "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_realA = (X_realA + 1) / 2.0\n",
    "\tX_realB = (X_realB + 1) / 2.0\n",
    "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
    "\t# plot real source images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(3, n_samples, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_realA[i])\n",
    "\t# plot generated target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_fakeB[i])\n",
    "\t# plot real target image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_realB[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pix2pix model\n",
    "def train(d_model, g_model, gan_model, base_model, dataset, n_epochs=2, n_batch=1):\n",
    "\t# determine the output square shape of the discriminator\n",
    "\tn_patch = d_model.output_shape[1]\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    # Freeze the weights of the VGG16 model\n",
    "   \n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\t\t# update discriminator for real samples\n",
    "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "\t\t# update discriminator for generated samples\n",
    "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "\t\t# update the generator\n",
    "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real,X_realB])\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\t\t# summarize model performance\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (4, 256, 256, 3) (4, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# load image data\n",
    "dataset = load_real_samples('multi.npz')\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-485-99ab02836c17>:27 ploss  *\n        feature_true = modelvgg(in_src)\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 256, 256, 3), found shape=(1, 16, 16, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-495-bbed44ac8604>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-492-9200ac4ca10d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(d_model, g_model, gan_model, dataset, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0md_loss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_fakeB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m# update the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# summarize performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>%d, d1[%.3f] d2[%.3f] g[%.3f]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1854\u001b[0m                                                     class_weight)\n\u001b[0;32m   1855\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1856\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 760\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-485-99ab02836c17>:27 ploss  *\n        feature_true = modelvgg(in_src)\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\probook\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 256, 256, 3), found shape=(1, 16, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)\n",
    "\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b23d018725ee08710807e1fe6dbd79d71300e70e1ead0b596e058ec39605bc38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
